# Embeddings

Embeddings is way of representing words by vectors that contain semantic meaning. 
A text may contain a massive number of unique words, trying to one-hot code these words is massively inefficient. If there are 50,000 unique words, only one of 50,000 elements is set to 1, all others are 0 for a specific word. The matrix multiplication going into the first hidden layer has almost all of the resulting values as zeros. That's computationally inefficient.

Embedings solve this problem by encoding each word with a much smaller length vector. The length of the embedding vector is called embedding dimension. If there are 50,000 unique words, there will be a fully connected layer with dimension 50,000 (one hot encode dimension) x 200 (embedding dimension). This weight matrix is called the embedding matrix. For each input word, rather than doing matrix multiplication, just the corresponding row from the embedding matrix is chosen, this process is called embedding lookup. This is equivalent to first hidden layer weight multiplication with one hot encoded inputs but computationally efficient.

Other than the comuputational efficiencies, this process has another use. Each 200 length vector is an unique representation of each word, and have semantical meaning. A fully connected layer can be trained with massive number of words, so that the semantically close words are represented by embedding vectors that are close to each other. Later, the embedding vector can be used as input to any other independent network. 

### Finding semantically close words to an input word

If two words are semantically close to each other in the context of the text used for training, then the two vectors representing the two words are also close to each other. Assume, two unit vectors in 2-D. V1 and V2. They are close if 
theta1 - theta2 is small (theta1 is the angle V1 makes with x axis, theta2 is the angle V2 makes with x axis) i.e. if cos(theta1-theta2) is close to 1. Therefore, the closeness of cos(theta1-theta2) to 1 is a measure of the two vector's closeness.
cos(theta1 - theta2) = cos(theta1) cos(theta2) + sin(theta1) sin(theta2). Each term represents multiplication of the compenets of the unit vectors along an axis. Therefore, if we normalize the embedding matrix (to make unit vector representation), select two rows represening two words and multiply the components along all dimensions and add, closer the result to 1, closer are the two words semantically. 
This is the logic used for finding close words to a particular word after training an embedding matrix. First the embedding matrix is normalized. The normal vector representing the input word is found. Each compenent of the vector is multipled with the same compnent of all other normal vectors (rows) and added. This is done by multiplying the input vector with the transpose of the embedding matrix , which automatically does the multiplication of same componets and then does the addition. In the resultant vector, closer an element to 1 , closer the corresponding word is to the input word.
