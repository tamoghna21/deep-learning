{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram word2vec\n",
    "\n",
    "TensorFlow implementation of word2vec algorithm using the skip-gram architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 22:17:21.519812: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [text8 dataset](http://mattmahoney.net/dc/textdata.html), a file of cleaned up Wikipedia articles from Matt Mahoney."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "\n",
    "dataset_folder_path = 'data'\n",
    "dataset_filename = 'text8.zip'\n",
    "dataset_name = 'Text8 Dataset'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(dataset_filename):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc=dataset_name) as pbar:\n",
    "        urlretrieve(\n",
    "            'http://mattmahoney.net/dc/text8.zip',\n",
    "            dataset_filename,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(dataset_folder_path):\n",
    "    with zipfile.ZipFile(dataset_filename) as zip_ref:\n",
    "        zip_ref.extractall(dataset_folder_path)\n",
    "        \n",
    "with open('data/text8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic instituti\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of different words in the vocabulary.\n",
    "max_vocabulary_size = 50000\n",
    "min_occurrence = 10  # Remove all words that does not appears at least n times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    words = text.lower().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess(text)\n",
    "print(tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 17005207\n",
      "Unique tokens: 253854\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens: {}\".format(len(tokens)))\n",
    "print(\"Unique tokens: {}\".format(len(set(tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47135\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Build the dictionary and replace rare words with UNK token.\n",
    "count = [('UNK', -1)]\n",
    "count.extend(Counter(tokens).most_common(max_vocabulary_size - 1))\n",
    "\n",
    "# Remove samples with less than 'min_occurrence' occurrences.\n",
    "for i in range(len(count) - 1, -1, -1):\n",
    "    if count[i][1] < min_occurrence:\n",
    "        count.pop(i)\n",
    "    else:\n",
    "        # The collection is ordered, so stop when 'min_occurrence' is reached.\n",
    "        break\n",
    "# Compute the vocabulary size.\n",
    "vocabulary_size = len(count)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens count: 17005207\n",
      "Unique tokens: 253854\n",
      "Vocabulary size: 47135\n",
      "Most common tokens: [('UNK', 444176), ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430)]\n"
     ]
    }
   ],
   "source": [
    "# Assign an id to each word.\n",
    "word2id = dict()\n",
    "for i, (word, _) in enumerate(count):\n",
    "    word2id[word] = i\n",
    "\n",
    "data = list()\n",
    "unk_count = 0\n",
    "for word in tokens:\n",
    "    # Retrieve a word id, or assign it index 0 ('UNK') if not in dictionary.\n",
    "    index = word2id.get(word, 0)\n",
    "    if index == 0:\n",
    "        unk_count += 1\n",
    "    data.append(index)\n",
    "count[0] = ('UNK', unk_count)\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "\n",
    "print(\"Tokens count:\", len(tokens))\n",
    "print(\"Unique tokens:\", len(set(tokens)))\n",
    "print(\"Vocabulary size:\", vocabulary_size)\n",
    "print(\"Most common tokens:\", count[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "data_index = 0\n",
    "# Generate training batch for the skip-gram model.\n",
    "def next_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    # get window size (words left and right + current one).\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch.\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Embedding\n",
    "The embedding matrix has a size of the number of words by the number of units in the hidden layer. So, if we have 10,000 words and 300 hidden units, the matrix will have size  10,000Ã—300 . Note that we're using tokenized data for our inputs, usually as integers, where the number of tokens is the number of words in our vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size =  200  # Number of embedding features\n",
    "\n",
    "num_sampled = 64  # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 22:18:24.135714: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "class MyModel(tf.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Create the embedding variable (each row represent a word embedding vector).\n",
    "        self.embedding = tf.Variable(tf.random.normal([vocabulary_size, embedding_size]))\n",
    "        # Construct the variables for the NCE loss.\n",
    "        self.nce_weights = tf.Variable(tf.random.normal([vocabulary_size, embedding_size]))\n",
    "        self.nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Lookup the corresponding embedding vectors for each sample in X.\n",
    "        return tf.nn.embedding_lookup(self.embedding, x)\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function with Negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every example we give the network, we train it using the output from the softmax layer. That means for each input, we're making very small changes to millions of weights even though we only have one true example. This makes training the network very inefficient. We can approximate the loss from the softmax layer by only updating a small subset of all the weights at once. We'll update the weights for the correct label, but only a small number of incorrect labels. This is called [\"negative sampling\"](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). \n",
    "We will use tensorflow function [`tf.nn.nce_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss) to do this. Compare this with [`tf.nn.sampled_softmax_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nce_loss_function(x_embed, y, nce_weights_,nce_biases_):\n",
    "    #with tf.device('/cpu:0'):\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights_,\n",
    "                    biases=nce_biases_,\n",
    "                    labels=y,\n",
    "                    inputs=x_embed,\n",
    "                    num_sampled=num_sampled,\n",
    "                    num_classes=vocabulary_size))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "\n",
    "Compute the cosine similarity between input data embedding and every embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,x_embed):\n",
    "    #with tf.device('/cpu:0'):\n",
    "    # Compute the cosine similarity between input data embedding and every embedding vectors\n",
    "    x_embed = tf.cast(x_embed, tf.float32)\n",
    "    x_embed_norm = x_embed / tf.sqrt(tf.reduce_sum(tf.square(x_embed)))\n",
    "    embedding_norm = model.embedding / \\\n",
    "        tf.sqrt(tf.reduce_sum(tf.square(model.embedding),\n",
    "                                1, keepdims=True), tf.float32)\n",
    "    cosine_sim_op = tf.matmul(\n",
    "        x_embed_norm, embedding_norm, transpose_b=True)\n",
    "    return cosine_sim_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, optimizer):\n",
    "    #with tf.device('/cpu:0'):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as t:\n",
    "        #emb = get_embedding(x)\n",
    "        emb = model(x)\n",
    "        loss = nce_loss_function(emb, y,model.nce_weights,model.nce_biases)\n",
    "\n",
    "    # Compute gradients.\n",
    "    gradients = t.gradient(loss, [model.embedding, model.nce_weights, model.nce_biases])\n",
    "\n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, [model.embedding, model.nce_weights, model.nce_biases]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Every display_step batches it reports the training loss. Every eval_step batches, it'll print out the validation words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  16    2 1344 1794   64  703]\n"
     ]
    }
   ],
   "source": [
    "# Words for testing.\n",
    "eval_words = ['five','of','going','hardware','american','britain']\n",
    "x_test = np.array([word2id[w] for w in eval_words])\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps  = 10000 #3000000 #10000\n",
    "display_step = 1000 # 10000 #1000\n",
    "eval_step = 2000 # 200000 #2000\n",
    "batch_size = 128\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "skip_window = 3  # How many words to consider left and right.\n",
    "\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss: 514.151794\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: quadrants, specials, bibles, zimmer, perfective, risings, clemency, naturalism,\n",
      "\"of\" nearest neighbors: decrypts, titian, dionysius, pardons, humanists, amazonas, floating, acceded,\n",
      "\"going\" nearest neighbors: honorable, sorghum, punishable, funk, swearing, don, bolstered, adverb,\n",
      "\"hardware\" nearest neighbors: irish, plazas, piloting, instigator, prospects, odour, ghetto, parke,\n",
      "\"american\" nearest neighbors: holt, stormed, alloying, redstone, outwardly, vecchio, rhyming, transitional,\n",
      "\"britain\" nearest neighbors: like, amstrad, mormonism, protector, cahiers, manchurian, undated, grateful,\n",
      "step: 1000, loss: 264.601135\n",
      "step: 2000, loss: 218.911957\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: two, is, and, in, one, a, six, for,\n",
      "\"of\" nearest neighbors: and, the, to, in, a, s, is, one,\n",
      "\"going\" nearest neighbors: honorable, funk, swearing, punishable, sorghum, don, renoir, adverb,\n",
      "\"hardware\" nearest neighbors: irish, plazas, piloting, instigator, kusakabe, ghetto, prospects, odour,\n",
      "\"american\" nearest neighbors: was, a, of, is, s, the, UNK, and,\n",
      "\"britain\" nearest neighbors: like, amstrad, mormonism, protector, cahiers, undated, manchurian, slip,\n",
      "step: 3000, loss: 211.126724\n",
      "step: 4000, loss: 235.816772\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: two, eight, three, one, four, six, nine, that,\n",
      "\"of\" nearest neighbors: the, and, in, a, to, as, is, by,\n",
      "\"going\" nearest neighbors: honorable, funk, don, swearing, sorghum, punishable, renoir, tors,\n",
      "\"hardware\" nearest neighbors: irish, plazas, piloting, instigator, kusakabe, ghetto, prospects, parke,\n",
      "\"american\" nearest neighbors: be, by, from, to, was, eight, zero, are,\n",
      "\"britain\" nearest neighbors: like, amstrad, mormonism, protector, cahiers, slip, outmoded, undated,\n",
      "step: 5000, loss: 162.361832\n",
      "step: 6000, loss: 140.685944\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: three, two, six, four, eight, seven, one, by,\n",
      "\"of\" nearest neighbors: and, the, a, in, to, that, from, for,\n",
      "\"going\" nearest neighbors: honorable, don, funk, swearing, renoir, punishable, sorghum, ment,\n",
      "\"hardware\" nearest neighbors: irish, plazas, piloting, kusakabe, chasm, instigator, ghetto, parke,\n",
      "\"american\" nearest neighbors: be, from, was, are, by, s, as, five,\n",
      "\"britain\" nearest neighbors: like, amstrad, mormonism, protector, outmoded, slip, cahiers, grateful,\n",
      "step: 7000, loss: 131.097534\n",
      "step: 8000, loss: 177.453506\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: four, three, two, by, with, seven, six, or,\n",
      "\"of\" nearest neighbors: the, and, in, a, is, that, by, to,\n",
      "\"going\" nearest neighbors: honorable, don, funk, swearing, renoir, punishable, sorghum, again,\n",
      "\"hardware\" nearest neighbors: irish, plazas, piloting, chasm, shield, kusakabe, parke, ghetto,\n",
      "\"american\" nearest neighbors: from, for, was, by, be, also, with, not,\n",
      "\"britain\" nearest neighbors: like, amstrad, what, mormonism, outmoded, films, name, protector,\n",
      "step: 9000, loss: 135.846252\n",
      "step: 10000, loss: 88.872093\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: four, from, two, six, eight, three, one, as,\n",
      "\"of\" nearest neighbors: the, and, in, a, to, as, by, was,\n",
      "\"going\" nearest neighbors: honorable, don, funk, system, again, renoir, swearing, oaxaca,\n",
      "\"hardware\" nearest neighbors: irish, plazas, piloting, chasm, ghetto, shield, kusakabe, parke,\n",
      "\"american\" nearest neighbors: be, four, seven, by, also, three, was, an,\n",
      "\"britain\" nearest neighbors: like, what, amstrad, name, states, world, about, films,\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Run training for the given number of steps.\n",
    "for step in range(1, num_steps + 1):\n",
    "    batch_x, batch_y = next_batch(batch_size, num_skips, skip_window)\n",
    "    train(model, batch_x, batch_y, optimizer)\n",
    "\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        loss = nce_loss_function(model(batch_x), batch_y,model.nce_weights,model.nce_biases)\n",
    "        print(\"step: %i, loss: %f\" % (step, loss))\n",
    "\n",
    "    # Evaluation.\n",
    "    if step % eval_step == 0 or step == 1:\n",
    "        print(\"Evaluation...\")\n",
    "        sim = evaluate(model, model(x_test)).numpy()\n",
    "        for i in range(len(eval_words)):\n",
    "            top_k = 8  # number of nearest neighbors.\n",
    "            nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "            log_str = '\"%s\" nearest neighbors:' % eval_words[i]\n",
    "            for k in range(top_k):\n",
    "                log_str = '%s %s,' % (log_str, id2word[nearest[k]])\n",
    "            print(log_str)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "* A really good [conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of word2vec from Chris McCormick \n",
    "* [First word2vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.\n",
    "* [NIPS paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) with improvements for word2vec also from Mikolov et al.\n",
    "* TensorFlow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)\n",
    "* [An implementation](https://github.com/sminerport/word2vec-skipgram-tensorflow/blob/master/src/word2vec.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
